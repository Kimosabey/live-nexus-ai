# LiveNexus AI

> **Real-Time Hybrid Audio Intelligence Platform**  
> CPU-Optimized Inference + Cloud Transport Architecture

[![Status](https://img.shields.io/badge/status-Phase%201%20Complete-success)]()
[![License](https://img.shields.io/badge/license-MIT-blue)]()
[![Next.js](https://img.shields.io/badge/Next.js-14-black)]()
[![Python](https://img.shields.io/badge/Python-3.10-blue)]()

---

## ğŸ“‹ Quick Start

### Prerequisites
- **Node.js** 18+ and npm
- **Python** 3.10+
- **Docker** (for AI worker)
- **LiveKit Cloud** account ([Get free tier](https://cloud.livekit.io))

### 1. Clone and Install

```bash
git clone https://github.com/Kimosabey/live-nexus-ai.git
cd live-nexus-ai
npm install
```

### 2. Configure LiveKit

Create `.env.local` in the project root:

```bash
LIVEKIT_API_KEY=your_api_key_here
LIVEKIT_API_SECRET=your_api_secret_here
LIVEKIT_URL=wss://your-project.livekit.cloud
```

> Get credentials from [LiveKit Cloud Dashboard](https://cloud.livekit.io/projects)

### 3. Run the Frontend

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

### 4. Run the AI Worker (Docker)

```bash
cd ai-worker
docker build -t livenexus-worker .
docker run --env-file ../.env.local livenexus-worker
```

---

## ğŸ“¸ Screenshots

### System Architecture
![LiveNexus AI Architecture](docs/assets/architecture-diagram.png)

### Real-Time Audio Pipeline
![Data Flow Diagram](docs/assets/data-flow-diagram.png)

### Core Features & Benefits
![Features Overview](docs/assets/features-overview.png)

### Technology Stack
![Tech Stack](docs/assets/tech-stack.png)

---

## ğŸ¯ Key Features

### Phase 1 (Current)
- âœ… **LiveKit Integration**: WebRTC audio streaming with cloud SFU
- âœ… **Stealth UI Design**: Pure black background, silver text, Space Grotesk font
- âœ… **Real-Time Connection**: Join/disconnect from LiveKit rooms
- âœ… **Python Worker Shell**: Connects as AI agent and logs audio reception
- âœ… **Double-Buffer Architecture**: Separate UI for partial/final transcripts

### Upcoming Phases
- ğŸ”„ **VAD Gating** (Phase 2): webrtcvad for 70% load reduction
- ğŸ”„ **Faster-Whisper** (Phase 2): CPU-optimized speech-to-text
- ğŸ”„ **Live Transcription** (Phase 3): Real-time text rendering with zero stutter
- ğŸ”„ **Resource Intelligence** (Phase 4): Auto-downgrade model at 80% CPU
- ğŸ”„ **Production Optimization** (Phase 5): Deployment guides and monitoring

---

## ğŸ—ï¸ Architecture

### System Flow

```mermaid
graph LR
    A[Next.js UI] -- "WebRTC Audio" --> B(LiveKit Cloud)
    B -- "Track Egress" --> C[Python AI Worker]
    C -- "VAD Filter" --> D{Human Speech?}
    D -- "Yes" --> E[Faster-Whisper CPU]
    D -- "No" --> F[Drop Packet]
    E -- "Text Chunk" --> G[LiveKit DataChannel]
    G -- "0ms Latency" --> A
```

### Layer Breakdown

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Next.js 14 + LiveKit Client SDK | Audio capture, transcript rendering |
| **Transport** | LiveKit Cloud (Free Tier) | WebRTC SFU, signaling, DataChannels |
| **AI Engine** | Python + Faster-Whisper | CPU-optimized speech-to-text |
| **VAD** | webrtcvad | Voice activity detection (70% load savings) |

### Performance Targets

- **Latency**: <200ms (audio â†’ transcript â†’ UI)
- **CPU Usage**: <60% average on i5 processors
- **Memory**: <16GB with base Whisper model
- **Throughput**: 5min continuous speech without degradation

---

## ğŸ§ª Testing & Scripts

### Development

```bash
# Frontend dev server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Lint code
npm run lint
```

### Worker Testing

```bash
# Build Docker image
cd ai-worker
docker build -t livenexus-worker .

# Run with environment variables
docker run --env-file ../.env.local livenexus-worker

# Check logs
docker logs <container_id>
```

### Manual Testing Checklist

1. âœ… Connect to LiveKit room
2. âœ… Allow microphone access
3. âœ… Verify worker logs show "Audio Received"
4. âœ… Check browser console for connection status
5. ğŸ”„ Speak and verify transcripts (Phase 2)

---

## ğŸ“š Documentation

### Project Structure

```
live-nexus-ai/
â”œâ”€â”€ app/                       # Next.js 14 App Router
â”‚   â”œâ”€â”€ api/livekit-token/    # JWT token generation
â”‚   â”œâ”€â”€ layout.tsx            # Root layout + SEO
â”‚   â”œâ”€â”€ page.tsx              # Main UI
â”‚   â””â”€â”€ globals.css           # Stealth design system
â”œâ”€â”€ components/               # React components
â”‚   â”œâ”€â”€ ConnectButton.tsx     # LiveKit room connector
â”‚   â””â”€â”€ TranscriptView.tsx    # Double-buffer transcripts
â”œâ”€â”€ ai-worker/                # Python AI Worker
â”‚   â”œâ”€â”€ Dockerfile            # Container definition
â”‚   â”œâ”€â”€ main.py              # Worker logic
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ README.md                # This file
```

### Senior Signals Implemented

1. **Zero-Stutter UI**: Separate React state for partial vs final transcripts
2. **Resource Intelligence**: Auto-downgrade Whisper model at 80% CPU (Phase 4)
3. **Binary Transport**: LiveKit DataChannels (not WebSockets)
4. **VAD Gating**: Only process human speech packets (Phase 2)

### Design System (Tab 13 - Stealth)

- **Background**: Pure Black (`#000000`)
- **Text**: Silver (`#C0C0C0`)
- **Accent**: Cyan (`#00D9FF`)
- **Typography**: Space Grotesk (Google Fonts)
- **Animation**: Subtle pulse effects for active states

---

## ğŸ› ï¸ Tech Stack

### Frontend
- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **WebRTC**: LiveKit Client SDK
- **Fonts**: Space Grotesk

### Backend
- **Runtime**: Python 3.10
- **AI**: Faster-Whisper (CTranslate2)
- **VAD**: webrtcvad
- **Async**: asyncio
- **Container**: Docker

### Infrastructure
- **Transport**: LiveKit Cloud (Free Tier)
- **Signaling**: WebRTC SFU
- **Data**: LiveKit DataChannels

---

## ğŸš€ Future Enhancements

### Phase 2: AI Integration (Week 2)
- [ ] Integrate webrtcvad for voice activity detection
- [ ] Add Faster-Whisper inference pipeline
- [ ] Implement audio format conversion (LiveKit â†’ Whisper)
- [ ] Test latency with different Whisper model sizes

### Phase 3: UI Polish (Week 3)
- [ ] Implement double-buffer transcript rendering
- [ ] Add timestamp formatting
- [ ] Create export functionality (JSON, TXT)
- [ ] Add error boundary components

### Phase 4: Resource Management (Week 4)
- [ ] CPU usage monitoring
- [ ] Auto-downgrade Whisper model logic
- [ ] Memory leak prevention
- [ ] Connection recovery logic

### Phase 5: Production Ready (Week 5)
- [ ] Add authentication layer
- [ ] Implement room-based isolation
- [ ] Create deployment guides (Docker Compose)
- [ ] Add telemetry and monitoring
- [ ] Write integration tests

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ‘¤ Author

**Harshan Aiyappa ** - Senior Hybrid Engineer  
Building Silicon Valley-grade AI platforms on consumer hardware

---

## ğŸ“ Learning Outcomes

This project demonstrates:
- **Real-Time Architecture**: WebRTC, SFU patterns, DataChannels
- **CPU-Optimized AI**: Running inference without GPU
- **Resource-Aware Design**: Adaptive model sizing, VAD filtering
- **Modern React**: Server Components, Suspense, TypeScript
- **Production Patterns**: Error handling, logging, monitoring

**Built with Antigravity Modeâ„¢** - Proving senior engineering is about architecture, not hardware.
